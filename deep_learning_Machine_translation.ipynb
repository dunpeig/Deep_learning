{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9025b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers.legacy import Adam \n",
    "Faker.seed(12345)\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0f7d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, RepeatVector, Concatenate, Dot, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "448932d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "14873621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']\n",
    "\n",
    "def load_date():\n",
    "\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',','')\n",
    "        machine_readable = dt.isoformat()\n",
    "\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "def load_dataset(m):\n",
    "    \"\"\"\n",
    "        Loads a dataset with m examples and vocabularies\n",
    "        :m: the number of examples to generate\n",
    "    \"\"\"\n",
    "\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30\n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    "\n",
    "    return dataset, human, machine, inv_machine\n",
    "\n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "\n",
    "    X, Y = zip(*dataset)\n",
    "\n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "\n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = string.replace(',','')\n",
    "\n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "\n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "\n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "\n",
    "    return rep\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "        \n",
    "\n",
    "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
    "\n",
    "    attention_map = np.zeros((10, 30))\n",
    "    Ty, Tx = attention_map.shape\n",
    "    \n",
    "    s0 = np.zeros((1, n_s))\n",
    "    c0 = np.zeros((1, n_s))\n",
    "    layer = model.layers[num]\n",
    "\n",
    "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
    "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
    "\n",
    "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    r = f([encoded, s0, c0])\n",
    "    \n",
    "    for t in range(Ty):\n",
    "        for t_prime in range(Tx):\n",
    "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
    "\n",
    "\n",
    "    prediction = model.predict([encoded, s0, c0])\n",
    "    \n",
    "    predicted_text = []\n",
    "    for i in range(len(prediction)):\n",
    "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
    "        \n",
    "    predicted_text = list(predicted_text)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "    text_ = list(text)\n",
    "    \n",
    "    input_length = len(text)\n",
    "    output_length = Ty\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    ax.grid()\n",
    "    \n",
    "    return attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06573d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee5490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, Tx=30, Ty=10, n_x = 32, n_y = 64):\n",
    "        self.model_param = {\n",
    "            \"Tx\": Tx, \n",
    "            \"Ty\": Ty,\n",
    "            \"n_x\": n_x,\n",
    "            \"n_y\": n_y\n",
    "        }\n",
    "\n",
    "\n",
    "    def load_data(self, m):\n",
    "        dataset, x_vocab, y_vocab, _ = load_dataset(m)\n",
    "\n",
    "        X, y, X_onehot, y_onehot = preprocess_data(dataset, x_vocab, y_vocab, self.model_param[\"Tx\"], self.model_param[\"Ty\"])\n",
    "\n",
    "        print(\"the data set feature shape: \", X_onehot.shape)\n",
    "        print(\"the data set target shape: \", y_onehot.shape)\n",
    "\n",
    "        print(\"see the first data frame: feature: %s, target: %s\" % (dataset[0][0], dataset[0][1]))\n",
    "        print(X[0], y[0])\n",
    "        print(\"one_got encode: \", X_onehot[0], y_onehot[0])\n",
    "\n",
    "        self.model_param[\"x_vocab\"] = x_vocab \n",
    "        self.model_param[\"y_vocab\"] = y_vocab \n",
    "\n",
    "        self.model_param[\"x_vocab_size\"] = len(x_vocab)\n",
    "        self.model_param[\"y_vocab_size\"] = len(y_vocab)\n",
    "        return X_onehot, y_onehot \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_encoder(self):\n",
    "        \n",
    "        #self.encoder = LSTM(self.model_param[\"n_x\"], return_sequences=True, name=\"bidirectional_1\", merge_mode=\"concat\")\n",
    "        self.encoder = Bidirectional(LSTM(self.model_param[\"n_x\"], return_sequences=True, name='bidirectional_1'), merge_mode='concat')\n",
    "\n",
    "        return None \n",
    "        \n",
    "    def get_decoder(self):\n",
    "        \n",
    "        self.decoder = LSTM(self.model_param[\"n_y\"], return_state=True)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_output_layer(self):\n",
    "        \n",
    "        self.output_layer = Dense(self.model_param[\"y_vocab_size\"], activation=softmax)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    def get_attention(self):\n",
    "        \n",
    "        repeator = RepeatVector(self.model_param[\"Tx\"])\n",
    "        \n",
    "        concatenator = Concatenate(axis=-1)\n",
    "        \n",
    "        densor1 = Dense(10, activation=\"tanh\", name=\"Dense1\")\n",
    "        \n",
    "        densor2 = Dense(1, activation=\"relu\", name=\"Dense2\")\n",
    "        \n",
    "        activator = Activation(softmax, name=\"attention_weights\")\n",
    "        \n",
    "        dotor = Dot(axes=1)\n",
    "        \n",
    "        self.attention = {\n",
    "            \"repeator\": repeator,\n",
    "            \"concatenator\": concatenator,\n",
    "            \"densor1\": densor1,\n",
    "            \"densor2\": densor2,\n",
    "            \"activator\": activator,\n",
    "            \"dotor\": dotor\n",
    "        }\n",
    "        \n",
    "        return None \n",
    "        \n",
    "    def init_seq2seq(self):\n",
    "        \n",
    "        self.get_encoder()\n",
    "        \n",
    "        self.get_decoder()\n",
    "        \n",
    "        self.get_attention()\n",
    "        \n",
    "        self.get_output_layer()\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    def computer_one_attention(self, a, s_prev):\n",
    "        \n",
    "        s_prev = self.attention[\"repeator\"](s_prev)\n",
    "        \n",
    "        concat = self.attention[\"concatenator\"]([a, s_prev])\n",
    "        \n",
    "        e = self.attention[\"densor1\"](concat)\n",
    "        \n",
    "        energies = self.attention[\"densor2\"](e)\n",
    "        \n",
    "        alphas = self.attention[\"activator\"](energies)\n",
    "        \n",
    "        context = self.attention[\"dotor\"]([alphas, a])\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    \n",
    "    def model(self):\n",
    "        \n",
    "        X = Input(shape=(self.model_param[\"Tx\"], self.model_param[\"x_vocab_size\"]), name=\"X\")\n",
    "        \n",
    "        s0 = Input(shape=(self.model_param[\"n_y\"],), name='s0')\n",
    "        \n",
    "        c0 = Input(shape=(self.model_param[\"n_y\"],),name='c0')\n",
    "        \n",
    "        s = s0\n",
    "        \n",
    "        c = c0 \n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        a = self.encoder(X)\n",
    "        \n",
    "        for t in range(self.model_param[\"Ty\"]):\n",
    "            \n",
    "            context = self.computer_one_attention(a, s)\n",
    "            \n",
    "            s, _, c = self.decoder(context, initial_state = [s,c])\n",
    "            \n",
    "            out = self.output_layer(s)\n",
    "            \n",
    "            outputs.append(out)\n",
    "            \n",
    "        model = Model(inputs=(X, s0, c0), outputs=outputs)\n",
    "        \n",
    "        return model \n",
    "        \n",
    "    def train(self, X_onehot, y_onehot):\n",
    "        \n",
    "        model = self.model()\n",
    "        \n",
    "        opt = Adam(lr= 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon=None, decay = 0.001)\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        s0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "        \n",
    "        c0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "        \n",
    "        outputs = list(y_onehot.swapaxes(0,1))\n",
    "        \n",
    "        model.fit([X_onehot, s0, c0], outputs, epochs = 1, batch_size=100)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "\n",
    "        model = self.model()\n",
    "\n",
    "        model.load_weights(\"./DL/model.h5\")\n",
    "\n",
    "        EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "        \n",
    "        for example in EXAMPLES:\n",
    "            \n",
    "            source = string_to_int(example, self.model_param[\"Tx\"], self.model_param[\"x_vocab\"])\n",
    "            source = np.array(list(map(lambda x: to_categorical(x, num_classes=self.model_param[\"x_vocab_size\"]), source)))\n",
    "                        \n",
    "            #s0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "            #c0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "            s0 = np.zeros((source.shape[0], self.model_param[\"n_y\"]))\n",
    "            c0 = np.zeros((source.shape[0], self.model_param[\"n_y\"]))\n",
    "            \n",
    "            prediction = model.predict([source, s0, c0])\n",
    "            \n",
    "            prediction = np.argmax(prediction, axis = -1)\n",
    "                        \n",
    "\n",
    "            \n",
    "            output = [dict(zip(self.model_param[\"y_vocab\"].values(), self.model_param[\"y_vocab\"].keys()))[int(i)] for i in prediction]\n",
    "\n",
    "            print(\"source:\", example)\n",
    "            print(\"output:\", ''.join(output))\n",
    "\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc79c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d136972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 22690.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data set feature shape:  (10000, 30, 37)\n",
      "the data set target shape:  (10000, 10, 11)\n",
      "see the first data frame: feature: 10 jul 1992, target: 1992-07-10\n",
      "[ 4  3  0 22 31 23  0  4 12 12  5 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36] [ 2 10 10  3  0  1  8  0  2  1]\n",
      "one_got encode:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Train on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 12:42:10.732199: W tensorflow/c/c_api.cc:304] Operation '{name:'training_10/Adam/lstm_25/lstm_cell/bias/m/Assign' id:115439 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/lstm_25/lstm_cell/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/lstm_25/lstm_cell/bias/m, training_10/Adam/lstm_25/lstm_cell/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 20.6736 - dense_22_loss: 2.0309 - dense_22_1_loss: 1.7218 - dense_22_2_loss: 2.2112 - dense_22_3_loss: 2.7748 - dense_22_4_loss: 1.3619 - dense_22_5_loss: 1.7232 - dense_22_6_loss: 2.7840 - dense_22_7_loss: 1.3727 - dense_22_8_loss: 1.9471 - dense_22_9_loss: 2.7460 - dense_22_accuracy: 0.0791 - dense_22_1_accuracy: 0.4107 - dense_22_2_accuracy: 0.1745 - dense_22_3_accuracy: 0.0798 - dense_22_4_accuracy: 0.7954 - dense_22_5_accuracy: 0.0685 - dense_22_6_accuracy: 0.0056 - dense_22_7_accuracy: 0.9526 - dense_22_8_accuracy: 0.0758 - dense_22_9_accuracy: 0.0437\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected X to have 3 dimensions, but got array with shape (30, 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m s2s\u001b[38;5;241m.\u001b[39minit_seq2seq()\n\u001b[1;32m      5\u001b[0m s2s\u001b[38;5;241m.\u001b[39mtrain(X_onehot, y_onehot)\n\u001b[0;32m----> 6\u001b[0m \u001b[43ms2s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36mSeq2seq.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m s0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((source\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    174\u001b[0m c0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((source\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m--> 176\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    182\u001b[0m output \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_param[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()))[\u001b[38;5;28mint\u001b[39m(i)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m prediction]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_v1.py:1059\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1058\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m-> 1059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_arrays_v1.py:798\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    789\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    796\u001b[0m ):\n\u001b[1;32m    797\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[0;32m--> 798\u001b[0m     x, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict_loop(\n\u001b[1;32m    802\u001b[0m         model,\n\u001b[1;32m    803\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    808\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_v1.py:2652\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly\n\u001b[1;32m   2645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_build_called\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)\n\u001b[1;32m   2649\u001b[0m ):\n\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_v1.py:2693\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[1;32m   2691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[1;32m   2692\u001b[0m     \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[0;32m-> 2693\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[1;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;66;03m# standardization code.\u001b[39;00m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_utils_v1.py:712\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    710\u001b[0m shape \u001b[38;5;241m=\u001b[39m shapes[i]\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape):\n\u001b[0;32m--> 712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when checking \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;241m+\u001b[39m exception_prefix\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;241m+\u001b[39m names[i]\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape))\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dimensions, but got array with shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_shape)\n\u001b[1;32m    721\u001b[0m     )\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_batch_axis:\n\u001b[1;32m    723\u001b[0m     data_shape \u001b[38;5;241m=\u001b[39m data_shape[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected X to have 3 dimensions, but got array with shape (30, 37)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    s2s = Seq2seq()\n",
    "    X_onehot, y_onehot = s2s.load_data(10000)\n",
    "    s2s.init_seq2seq()\n",
    "    s2s.train(X_onehot, y_onehot)\n",
    "    s2s.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb32a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6116a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07781b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c916ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b2dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
