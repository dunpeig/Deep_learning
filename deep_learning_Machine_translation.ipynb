{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9025b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers.legacy import Adam \n",
    "Faker.seed(12345)\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0f7d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, RepeatVector, Concatenate, Dot, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "448932d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "14873621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']\n",
    "\n",
    "def load_date():\n",
    "\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',','')\n",
    "        machine_readable = dt.isoformat()\n",
    "\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "def load_dataset(m):\n",
    "    \"\"\"\n",
    "        Loads a dataset with m examples and vocabularies\n",
    "        :m: the number of examples to generate\n",
    "    \"\"\"\n",
    "\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30\n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    "\n",
    "    return dataset, human, machine, inv_machine\n",
    "\n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "\n",
    "    X, Y = zip(*dataset)\n",
    "\n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "\n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = string.replace(',','')\n",
    "\n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "\n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "\n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "\n",
    "    return rep\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "        \n",
    "\n",
    "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
    "\n",
    "    attention_map = np.zeros((10, 30))\n",
    "    Ty, Tx = attention_map.shape\n",
    "    \n",
    "    s0 = np.zeros((1, n_s))\n",
    "    c0 = np.zeros((1, n_s))\n",
    "    layer = model.layers[num]\n",
    "\n",
    "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
    "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
    "\n",
    "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    r = f([encoded, s0, c0])\n",
    "    \n",
    "    for t in range(Ty):\n",
    "        for t_prime in range(Tx):\n",
    "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
    "\n",
    "\n",
    "    prediction = model.predict([encoded, s0, c0])\n",
    "    \n",
    "    predicted_text = []\n",
    "    for i in range(len(prediction)):\n",
    "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
    "        \n",
    "    predicted_text = list(predicted_text)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "    text_ = list(text)\n",
    "    \n",
    "    input_length = len(text)\n",
    "    output_length = Ty\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    ax.grid()\n",
    "    \n",
    "    return attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06573d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee5490b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, Tx=30, Ty=10, n_x = 32, n_y = 64):\n",
    "        self.model_param = {\n",
    "            \"Tx\": Tx, \n",
    "            \"Ty\": Ty,\n",
    "            \"n_x\": n_x,\n",
    "            \"n_y\": n_y\n",
    "        }\n",
    "\n",
    "\n",
    "    def load_data(self, m):\n",
    "        dataset, x_vocab, y_vocab, _ = load_dataset(m)\n",
    "\n",
    "        X, y, X_onehot, y_onehot = preprocess_data(dataset, x_vocab, y_vocab, self.model_param[\"Tx\"], self.model_param[\"Ty\"])\n",
    "\n",
    "        print(\"the data set feature shape: \", X_onehot.shape)\n",
    "        print(\"the data set target shape: \", y_onehot.shape)\n",
    "\n",
    "        print(\"see the first data frame: feature: %s, target: %s\" % (dataset[0][0], dataset[0][1]))\n",
    "        print(X[0], y[0])\n",
    "        print(\"one_got encode: \", X_onehot[0], y_onehot[0])\n",
    "\n",
    "        self.model_param[\"x_vocab\"] = x_vocab \n",
    "        self.model_param[\"y_vocab\"] = y_vocab \n",
    "\n",
    "        self.model_param[\"x_vocab_size\"] = len(x_vocab)\n",
    "        self.model_param[\"y_vocab_size\"] = len(y_vocab)\n",
    "        return X_onehot, y_onehot \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_encoder(self):\n",
    "        \n",
    "        #self.encoder = LSTM(self.model_param[\"n_x\"], return_sequences=True, name=\"bidirectional_1\", merge_mode=\"concat\")\n",
    "        self.encoder = Bidirectional(LSTM(self.model_param[\"n_x\"], return_sequences=True, name='bidirectional_1'), merge_mode='concat')\n",
    "\n",
    "        return None \n",
    "        \n",
    "    def get_decoder(self):\n",
    "        \n",
    "        self.decoder = LSTM(self.model_param[\"n_y\"], return_state=True)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_output_layer(self):\n",
    "        \n",
    "        self.output_layer = Dense(self.model_param[\"y_vocab_size\"], activation=softmax)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    def get_attention(self):\n",
    "        \n",
    "        repeator = RepeatVector(self.model_param[\"Tx\"])\n",
    "        \n",
    "        concatenator = Concatenate(axis=-1)\n",
    "        \n",
    "        densor1 = Dense(10, activation=\"tanh\", name=\"Dense1\")\n",
    "        \n",
    "        densor2 = Dense(1, activation=\"relu\", name=\"Dense2\")\n",
    "        \n",
    "        activator = Activation(softmax, name=\"attention_weights\")\n",
    "        \n",
    "        dotor = Dot(axes=1)\n",
    "        \n",
    "        self.attention = {\n",
    "            \"repeator\": repeator,\n",
    "            \"concatenator\": concatenator,\n",
    "            \"densor1\": densor1,\n",
    "            \"densor2\": densor2,\n",
    "            \"activator\": activator,\n",
    "            \"dotor\": dotor\n",
    "        }\n",
    "        \n",
    "        return None \n",
    "        \n",
    "    def init_seq2seq(self):\n",
    "        \n",
    "        self.get_encoder()\n",
    "        \n",
    "        self.get_decoder()\n",
    "        \n",
    "        self.get_attention()\n",
    "        \n",
    "        self.get_output_layer()\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    def computer_one_attention(self, a, s_prev):\n",
    "        \n",
    "        s_prev = self.attention[\"repeator\"](s_prev)\n",
    "        \n",
    "        concat = self.attention[\"concatenator\"]([a, s_prev])\n",
    "        \n",
    "        e = self.attention[\"densor1\"](concat)\n",
    "        \n",
    "        energies = self.attention[\"densor2\"](e)\n",
    "        \n",
    "        alphas = self.attention[\"activator\"](energies)\n",
    "        \n",
    "        context = self.attention[\"dotor\"]([alphas, a])\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    \n",
    "    def model(self):\n",
    "        \n",
    "        X = Input(shape=(self.model_param[\"Tx\"], self.model_param[\"x_vocab_size\"]), name=\"X\")\n",
    "        \n",
    "        s0 = Input(shape=(self.model_param[\"n_y\"],), name='s0')\n",
    "        \n",
    "        c0 = Input(shape=(self.model_param[\"n_y\"],),name='c0')\n",
    "        \n",
    "        s = s0\n",
    "        \n",
    "        c = c0 \n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        a = self.encoder(X)\n",
    "        \n",
    "        for t in range(self.model_param[\"Ty\"]):\n",
    "            \n",
    "            context = self.computer_one_attention(a, s)\n",
    "            \n",
    "            s, _, c = self.decoder(context, initial_state = [s,c])\n",
    "            \n",
    "            out = self.output_layer(s)\n",
    "            \n",
    "            outputs.append(out)\n",
    "            \n",
    "        model = Model(inputs=(X, s0, c0), outputs=outputs)\n",
    "        \n",
    "        return model \n",
    "        \n",
    "    def train(self, X_onehot, y_onehot):\n",
    "        \n",
    "        model = self.model()\n",
    "        \n",
    "        opt = Adam(lr= 0.005, beta_1 = 0.9, beta_2 = 0.999, epsilon=None, decay = 0.001)\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        s0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "        \n",
    "        c0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "        \n",
    "        outputs = list(y_onehot.swapaxes(0,1))\n",
    "        \n",
    "        model.fit([X_onehot, s0, c0], outputs, epochs = 1, batch_size=100)\n",
    "        \n",
    "        return None \n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "\n",
    "        model = self.model()\n",
    "\n",
    "        model.load_weights(\"./DL/model.h5\")\n",
    "\n",
    "        EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "        \n",
    "        for example in EXAMPLES:\n",
    "            \n",
    "            source = string_to_int(example, self.model_param[\"Tx\"], self.model_param[\"x_vocab\"])\n",
    "            source = np.array(list(map(lambda x: to_categorical(x, num_classes=self.model_param[\"x_vocab_size\"]), source)))\n",
    "                        \n",
    "            source = np.expand_dims(source, axis=0)\n",
    "            #s0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "            #c0 = np.zeros((10000, self.model_param[\"n_y\"]))\n",
    "            s0 = np.zeros((source.shape[0], self.model_param[\"n_y\"]))\n",
    "            c0 = np.zeros((source.shape[0], self.model_param[\"n_y\"]))\n",
    "            \n",
    "            prediction = model.predict([source, s0, c0])\n",
    "            \n",
    "            prediction = np.argmax(prediction, axis = -1)\n",
    "                        \n",
    "\n",
    "            \n",
    "            output = [dict(zip(self.model_param[\"y_vocab\"].values(), self.model_param[\"y_vocab\"].keys()))[int(i)] for i in prediction]\n",
    "\n",
    "            print(\"source:\", example)\n",
    "            print(\"output:\", ''.join(output))\n",
    "\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc79c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d136972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 13726.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data set feature shape:  (10000, 30, 37)\n",
      "the data set target shape:  (10000, 10, 11)\n",
      "see the first data frame: feature: sunday january 20 2013, target: 2013-01-20\n",
      "[29 31 25 16 13 34  0 22 13 25 31 13 28 34  0  5  3  0  5  3  4  6 36 36\n",
      " 36 36 36 36 36 36] [3 1 2 4 0 1 2 0 3 1]\n",
      "one_got encode:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Train on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:38:45.621698: W tensorflow/c/c_api.cc:304] Operation '{name:'count_151/Assign' id:129079 op device:{requested: '', assigned: ''} def:{{{node count_151/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_151, count_151/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 20.7841 - dense_25_loss: 2.0873 - dense_25_1_loss: 1.8146 - dense_25_2_loss: 2.2711 - dense_25_3_loss: 2.7493 - dense_25_4_loss: 1.3929 - dense_25_5_loss: 1.6850 - dense_25_6_loss: 2.7431 - dense_25_7_loss: 1.3970 - dense_25_8_loss: 1.9075 - dense_25_9_loss: 2.7363 - dense_25_accuracy: 0.0629 - dense_25_1_accuracy: 0.3976 - dense_25_2_accuracy: 0.1604 - dense_25_3_accuracy: 0.0701 - dense_25_4_accuracy: 0.8147 - dense_25_5_accuracy: 0.0830 - dense_25_6_accuracy: 0.0109 - dense_25_7_accuracy: 0.8369 - dense_25_8_accuracy: 0.1589 - dense_25_9_accuracy: 0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 21:43:06.070821: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_25_10/Softmax' id:133536 op device:{requested: '', assigned: ''} def:{{{node dense_25_10/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_25_10/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-33\n",
      "source: 5 April 09\n",
      "output: 2009-04-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    s2s = Seq2seq()\n",
    "    X_onehot, y_onehot = s2s.load_data(10000)\n",
    "    s2s.init_seq2seq()\n",
    "    s2s.train(X_onehot, y_onehot)\n",
    "    s2s.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb32a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6116a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07781b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c916ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b2dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
